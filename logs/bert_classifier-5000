[2024-04-13 13:17:16,118 INFO] Device ID 0
[2024-04-13 13:17:16,119 INFO] Device cuda
[2024-04-13 13:17:44,742 ERROR] Model name 'bert-base-uncased' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese). We assumed 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz' was a path or url but couldn't find any file associated to this path or url.
[2024-04-13 13:18:02,232 INFO] Device ID 0
[2024-04-13 13:18:02,232 INFO] Device cuda
[2024-04-13 13:18:02,261 ERROR] Model name 'bert-base-uncased' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese). We assumed 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz' was a path or url but couldn't find any file associated to this path or url.
[2024-04-13 13:18:38,355 INFO] Device ID 0
[2024-04-13 13:18:38,355 INFO] Device cuda
[2024-04-13 13:18:38,403 ERROR] Model name 'bert-base-uncased' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese). We assumed 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz' was a path or url but couldn't find any file associated to this path or url.
[2024-04-13 13:19:32,228 INFO] Device ID 0
[2024-04-13 13:19:32,228 INFO] Device cuda
[2024-04-13 13:19:32,257 ERROR] Model name 'bert-base-uncased' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese). We assumed 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz' was a path or url but couldn't find any file associated to this path or url.
[2024-04-13 13:21:45,771 INFO] Device ID 0
[2024-04-13 13:21:45,771 INFO] Device cuda
[2024-04-13 13:21:45,800 ERROR] Model name 'bert-base-uncased' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese). We assumed 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz' was a path or url but couldn't find any file associated to this path or url.
[2024-04-13 13:22:02,257 INFO] Device ID 0
[2024-04-13 13:22:02,258 INFO] Device cuda
[2024-04-13 13:22:02,286 ERROR] Model name 'bert-base-uncased' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese). We assumed 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz' was a path or url but couldn't find any file associated to this path or url.
[2024-04-13 13:22:14,365 INFO] Device ID 0
[2024-04-13 13:22:14,365 INFO] Device cuda
[2024-04-13 13:22:14,394 ERROR] Model name 'bert-base-uncased' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese). We assumed 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz' was a path or url but couldn't find any file associated to this path or url.
[2024-04-13 15:06:12,836 INFO] Device ID 0
[2024-04-13 15:06:12,845 INFO] Device cuda
[2024-04-13 15:06:12,875 ERROR] Model name 'bert-base-uncased' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese). We assumed 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz' was a path or url but couldn't find any file associated to this path or url.
[2024-04-13 15:06:56,960 INFO] Device ID 0
[2024-04-13 15:06:56,960 INFO] Device cuda
[2024-04-13 15:06:56,989 ERROR] Model name 'bert-base-uncased' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese). We assumed 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz' was a path or url but couldn't find any file associated to this path or url.
[2024-04-13 15:12:03,963 INFO] Device ID 0
[2024-04-13 15:12:03,963 INFO] Device cuda
[2024-04-13 15:12:04,010 ERROR] Model name 'bert-base-uncased' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese). We assumed 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz' was a path or url but couldn't find any file associated to this path or url.
[2024-04-13 17:14:44,489 INFO] Device ID 0
[2024-04-13 17:14:44,489 INFO] Device cuda
[2024-04-13 17:14:44,518 ERROR] Model name 'bert-base-uncased' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese). We assumed 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz' was a path or url but couldn't find any file associated to this path or url.
[2024-04-13 17:17:26,037 INFO] Device ID 0
[2024-04-13 17:17:26,037 INFO] Device cuda
[2024-04-13 17:17:27,111 INFO] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz not found in cache, downloading to /tmp/tmp9sk_hqq_
[2024-04-13 17:39:04,170 INFO] copying /tmp/tmp9sk_hqq_ to cache at ../temp/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba
[2024-04-13 17:39:04,357 INFO] creating metadata file for ../temp/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba
[2024-04-13 17:39:04,357 INFO] removing temp file /tmp/tmp9sk_hqq_
[2024-04-13 17:39:04,386 INFO] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at ../temp/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba
[2024-04-13 17:39:04,386 INFO] extracting archive file ../temp/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpx8mzr1tl
[2024-04-13 17:39:06,368 INFO] Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2024-04-13 17:39:07,841 INFO] Summarizer(
  (bert): Bert(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): BertLayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): BertLayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (encoder): Classifier(
    (linear1): Linear(in_features=768, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
[2024-04-13 17:39:07,844 INFO] * number of parameters: 109483009
[2024-04-13 17:39:07,844 INFO] Start training...
[2024-04-13 17:39:07,906 INFO] Loading train dataset from /home/wjm/hrx/BertSum_cnndm/bert_data/cnndm.train.123.bert.pt, number of examples: 2001
[2024-04-13 17:39:15,901 INFO] Step 50/ 5000; xent: 9.79; lr: 0.0000032;  32 docs/s;      8 sec
[2024-04-13 17:39:23,387 INFO] Step 100/ 5000; xent: 6.44; lr: 0.0000063;  33 docs/s;     15 sec
[2024-04-13 17:39:30,988 INFO] Step 150/ 5000; xent: 6.60; lr: 0.0000095;  33 docs/s;     23 sec
[2024-04-13 17:39:38,523 INFO] Step 200/ 5000; xent: 6.62; lr: 0.0000126;  33 docs/s;     31 sec
[2024-04-13 17:39:46,155 INFO] Step 250/ 5000; xent: 6.57; lr: 0.0000158;  33 docs/s;     38 sec
[2024-04-13 17:39:53,744 INFO] Step 300/ 5000; xent: 6.58; lr: 0.0000190;  33 docs/s;     46 sec
[2024-04-13 17:40:01,512 INFO] Step 350/ 5000; xent: 6.45; lr: 0.0000221;  32 docs/s;     54 sec
[2024-04-13 17:40:09,327 INFO] Loading train dataset from /home/wjm/hrx/BertSum_cnndm/bert_data/cnndm.train.91.bert.pt, number of examples: 1998
[2024-04-13 17:40:09,663 INFO] Step 400/ 5000; xent: 6.40; lr: 0.0000253;  31 docs/s;     62 sec
[2024-04-13 17:40:17,578 INFO] Step 450/ 5000; xent: 6.33; lr: 0.0000285;  32 docs/s;     70 sec
[2024-04-13 17:40:25,403 INFO] Step 500/ 5000; xent: 6.07; lr: 0.0000316;  32 docs/s;     77 sec
[2024-04-13 17:40:33,389 INFO] Step 550/ 5000; xent: 6.24; lr: 0.0000348;  32 docs/s;     85 sec
[2024-04-13 17:40:41,335 INFO] Step 600/ 5000; xent: 5.97; lr: 0.0000379;  31 docs/s;     93 sec
[2024-04-13 17:40:49,086 INFO] Step 650/ 5000; xent: 5.92; lr: 0.0000411;  33 docs/s;    101 sec
[2024-04-13 17:40:57,008 INFO] Step 700/ 5000; xent: 6.11; lr: 0.0000443;  32 docs/s;    109 sec
[2024-04-13 17:41:04,915 INFO] Step 750/ 5000; xent: 5.98; lr: 0.0000474;  31 docs/s;    117 sec
[2024-04-13 17:41:12,520 INFO] Loading train dataset from /home/wjm/hrx/BertSum_cnndm/bert_data/cnndm.train.39.bert.pt, number of examples: 2000
[2024-04-13 17:41:12,999 INFO] Step 800/ 5000; xent: 5.85; lr: 0.0000506;  31 docs/s;    125 sec
[2024-04-13 17:41:20,981 INFO] Step 850/ 5000; xent: 5.99; lr: 0.0000538;  31 docs/s;    133 sec
[2024-04-13 17:41:28,905 INFO] Step 900/ 5000; xent: 6.22; lr: 0.0000569;  32 docs/s;    141 sec
[2024-04-13 17:41:36,895 INFO] Step 950/ 5000; xent: 6.34; lr: 0.0000601;  31 docs/s;    149 sec
[2024-04-13 17:41:44,793 INFO] Step 1000/ 5000; xent: 5.88; lr: 0.0000632;  32 docs/s;    157 sec
[2024-04-13 17:41:44,795 INFO] Saving checkpoint /home/wjm/hrx/BertSum_cnndm/models/bert_classifier_5000/model_step_1000.pt
[2024-04-13 17:41:54,005 INFO] Step 1050/ 5000; xent: 6.10; lr: 0.0000617;  27 docs/s;    166 sec
[2024-04-13 17:42:01,930 INFO] Step 1100/ 5000; xent: 5.89; lr: 0.0000603;  32 docs/s;    174 sec
[2024-04-13 17:42:09,929 INFO] Step 1150/ 5000; xent: 5.84; lr: 0.0000590;  32 docs/s;    182 sec
[2024-04-13 17:42:17,073 INFO] Loading train dataset from /home/wjm/hrx/BertSum_cnndm/bert_data/cnndm.train.6.bert.pt, number of examples: 2001
[2024-04-13 17:42:18,047 INFO] Step 1200/ 5000; xent: 6.24; lr: 0.0000577;  31 docs/s;    190 sec
[2024-04-13 17:42:26,007 INFO] Step 1250/ 5000; xent: 6.18; lr: 0.0000566;  32 docs/s;    198 sec
[2024-04-13 17:42:33,920 INFO] Step 1300/ 5000; xent: 6.10; lr: 0.0000555;  32 docs/s;    206 sec
[2024-04-13 17:42:41,908 INFO] Step 1350/ 5000; xent: 5.79; lr: 0.0000544;  32 docs/s;    214 sec
[2024-04-13 17:42:49,597 INFO] Step 1400/ 5000; xent: 6.17; lr: 0.0000535;  32 docs/s;    222 sec
[2024-04-13 17:42:57,192 INFO] Step 1450/ 5000; xent: 5.94; lr: 0.0000525;  33 docs/s;    229 sec
[2024-04-13 17:43:04,879 INFO] Step 1500/ 5000; xent: 6.03; lr: 0.0000516;  33 docs/s;    237 sec
[2024-04-13 17:43:12,557 INFO] Step 1550/ 5000; xent: 5.79; lr: 0.0000508;  33 docs/s;    245 sec
[2024-04-13 17:43:18,945 INFO] Loading train dataset from /home/wjm/hrx/BertSum_cnndm/bert_data/cnndm.train.81.bert.pt, number of examples: 2000
[2024-04-13 17:43:20,335 INFO] Step 1600/ 5000; xent: 5.88; lr: 0.0000500;  33 docs/s;    252 sec
[2024-04-13 17:43:27,970 INFO] Step 1650/ 5000; xent: 5.76; lr: 0.0000492;  33 docs/s;    260 sec
[2024-04-13 17:43:35,657 INFO] Step 1700/ 5000; xent: 5.93; lr: 0.0000485;  33 docs/s;    268 sec
[2024-04-13 17:43:43,263 INFO] Step 1750/ 5000; xent: 5.67; lr: 0.0000478;  33 docs/s;    275 sec
[2024-04-13 17:43:50,861 INFO] Step 1800/ 5000; xent: 5.84; lr: 0.0000471;  33 docs/s;    283 sec
[2024-04-13 17:43:58,569 INFO] Step 1850/ 5000; xent: 6.00; lr: 0.0000465;  33 docs/s;    291 sec
[2024-04-13 17:44:06,154 INFO] Step 1900/ 5000; xent: 5.48; lr: 0.0000459;  33 docs/s;    298 sec
[2024-04-13 17:44:13,824 INFO] Step 1950/ 5000; xent: 5.64; lr: 0.0000453;  34 docs/s;    306 sec
[2024-04-13 17:44:19,779 INFO] Loading train dataset from /home/wjm/hrx/BertSum_cnndm/bert_data/cnndm.train.98.bert.pt, number of examples: 2000
[2024-04-13 17:44:21,653 INFO] Step 2000/ 5000; xent: 5.79; lr: 0.0000447;  32 docs/s;    314 sec
[2024-04-13 17:44:21,655 INFO] Saving checkpoint /home/wjm/hrx/BertSum_cnndm/models/bert_classifier_5000/model_step_2000.pt
[2024-04-13 17:44:30,296 INFO] Step 2050/ 5000; xent: 6.07; lr: 0.0000442;  29 docs/s;    322 sec
[2024-04-13 17:44:37,973 INFO] Step 2100/ 5000; xent: 6.07; lr: 0.0000436;  33 docs/s;    330 sec
[2024-04-13 17:44:45,582 INFO] Step 2150/ 5000; xent: 5.91; lr: 0.0000431;  33 docs/s;    338 sec
[2024-04-13 17:44:53,294 INFO] Step 2200/ 5000; xent: 5.59; lr: 0.0000426;  33 docs/s;    345 sec
[2024-04-13 17:45:00,968 INFO] Step 2250/ 5000; xent: 6.01; lr: 0.0000422;  33 docs/s;    353 sec
[2024-04-13 17:45:08,600 INFO] Step 2300/ 5000; xent: 5.96; lr: 0.0000417;  33 docs/s;    361 sec
[2024-04-13 17:45:16,212 INFO] Step 2350/ 5000; xent: 5.93; lr: 0.0000413;  33 docs/s;    368 sec
[2024-04-13 17:45:21,719 INFO] Loading train dataset from /home/wjm/hrx/BertSum_cnndm/bert_data/cnndm.train.93.bert.pt, number of examples: 1997
[2024-04-13 17:45:24,036 INFO] Step 2400/ 5000; xent: 5.69; lr: 0.0000408;  33 docs/s;    376 sec
[2024-04-13 17:45:31,721 INFO] Step 2450/ 5000; xent: 5.85; lr: 0.0000404;  33 docs/s;    384 sec
[2024-04-13 17:45:39,335 INFO] Step 2500/ 5000; xent: 5.71; lr: 0.0000400;  33 docs/s;    391 sec
[2024-04-13 17:45:46,976 INFO] Step 2550/ 5000; xent: 5.74; lr: 0.0000396;  33 docs/s;    399 sec
[2024-04-13 17:45:54,619 INFO] Step 2600/ 5000; xent: 5.84; lr: 0.0000392;  33 docs/s;    407 sec
[2024-04-13 17:46:02,302 INFO] Step 2650/ 5000; xent: 5.60; lr: 0.0000389;  33 docs/s;    414 sec
[2024-04-13 17:46:09,842 INFO] Step 2700/ 5000; xent: 5.91; lr: 0.0000385;  33 docs/s;    422 sec
[2024-04-13 17:46:17,571 INFO] Step 2750/ 5000; xent: 5.79; lr: 0.0000381;  33 docs/s;    430 sec
[2024-04-13 17:46:22,481 INFO] Loading train dataset from /home/wjm/hrx/BertSum_cnndm/bert_data/cnndm.train.63.bert.pt, number of examples: 2001
[2024-04-13 17:46:25,410 INFO] Step 2800/ 5000; xent: 5.65; lr: 0.0000378;  32 docs/s;    438 sec
[2024-04-13 17:46:32,965 INFO] Step 2850/ 5000; xent: 5.62; lr: 0.0000375;  33 docs/s;    445 sec
[2024-04-13 17:46:40,654 INFO] Step 2900/ 5000; xent: 5.70; lr: 0.0000371;  33 docs/s;    453 sec
[2024-04-13 17:46:48,236 INFO] Step 2950/ 5000; xent: 5.73; lr: 0.0000368;  33 docs/s;    460 sec
[2024-04-13 17:46:55,898 INFO] Step 3000/ 5000; xent: 5.70; lr: 0.0000365;  33 docs/s;    468 sec
[2024-04-13 17:46:55,900 INFO] Saving checkpoint /home/wjm/hrx/BertSum_cnndm/models/bert_classifier_5000/model_step_3000.pt
[2024-04-13 17:47:04,496 INFO] Step 3050/ 5000; xent: 5.78; lr: 0.0000362;  29 docs/s;    477 sec
[2024-04-13 17:47:12,231 INFO] Step 3100/ 5000; xent: 5.79; lr: 0.0000359;  34 docs/s;    484 sec
[2024-04-13 17:47:19,754 INFO] Step 3150/ 5000; xent: 5.91; lr: 0.0000356;  33 docs/s;    492 sec
[2024-04-13 17:47:24,195 INFO] Loading train dataset from /home/wjm/hrx/BertSum_cnndm/bert_data/cnndm.train.15.bert.pt, number of examples: 1999
[2024-04-13 17:47:27,584 INFO] Step 3200/ 5000; xent: 5.83; lr: 0.0000354;  32 docs/s;    500 sec
[2024-04-13 17:47:35,115 INFO] Step 3250/ 5000; xent: 5.68; lr: 0.0000351;  33 docs/s;    507 sec
[2024-04-13 17:47:42,735 INFO] Step 3300/ 5000; xent: 5.83; lr: 0.0000348;  33 docs/s;    515 sec
[2024-04-13 17:47:50,363 INFO] Step 3350/ 5000; xent: 5.76; lr: 0.0000346;  33 docs/s;    522 sec
[2024-04-13 17:47:58,046 INFO] Step 3400/ 5000; xent: 5.78; lr: 0.0000343;  33 docs/s;    530 sec
[2024-04-13 17:48:05,718 INFO] Step 3450/ 5000; xent: 5.53; lr: 0.0000341;  33 docs/s;    538 sec
[2024-04-13 17:48:13,244 INFO] Step 3500/ 5000; xent: 5.94; lr: 0.0000338;  33 docs/s;    545 sec
[2024-04-13 17:48:20,846 INFO] Step 3550/ 5000; xent: 5.63; lr: 0.0000336;  33 docs/s;    553 sec
[2024-04-13 17:48:24,892 INFO] Loading train dataset from /home/wjm/hrx/BertSum_cnndm/bert_data/cnndm.train.64.bert.pt, number of examples: 2001
[2024-04-13 17:48:28,596 INFO] Step 3600/ 5000; xent: 5.77; lr: 0.0000333;  33 docs/s;    561 sec
[2024-04-13 17:48:36,283 INFO] Step 3650/ 5000; xent: 5.78; lr: 0.0000331;  33 docs/s;    568 sec
[2024-04-13 17:48:43,865 INFO] Step 3700/ 5000; xent: 6.00; lr: 0.0000329;  33 docs/s;    576 sec
[2024-04-13 17:48:51,560 INFO] Step 3750/ 5000; xent: 5.73; lr: 0.0000327;  33 docs/s;    584 sec
[2024-04-13 17:48:59,237 INFO] Step 3800/ 5000; xent: 5.58; lr: 0.0000324;  33 docs/s;    591 sec
[2024-04-13 17:49:06,773 INFO] Step 3850/ 5000; xent: 5.85; lr: 0.0000322;  33 docs/s;    599 sec
[2024-04-13 17:49:14,472 INFO] Step 3900/ 5000; xent: 5.50; lr: 0.0000320;  33 docs/s;    607 sec
[2024-04-13 17:49:22,094 INFO] Step 3950/ 5000; xent: 5.67; lr: 0.0000318;  33 docs/s;    614 sec
[2024-04-13 17:49:25,750 INFO] Loading train dataset from /home/wjm/hrx/BertSum_cnndm/bert_data/cnndm.train.28.bert.pt, number of examples: 2000
[2024-04-13 17:49:29,902 INFO] Step 4000/ 5000; xent: 5.70; lr: 0.0000316;  32 docs/s;    622 sec
[2024-04-13 17:49:29,904 INFO] Saving checkpoint /home/wjm/hrx/BertSum_cnndm/models/bert_classifier_5000/model_step_4000.pt
[2024-04-13 17:49:38,554 INFO] Step 4050/ 5000; xent: 5.53; lr: 0.0000314;  30 docs/s;    631 sec
[2024-04-13 17:49:46,186 INFO] Step 4100/ 5000; xent: 5.91; lr: 0.0000312;  33 docs/s;    638 sec
[2024-04-13 17:49:53,809 INFO] Step 4150/ 5000; xent: 5.93; lr: 0.0000310;  33 docs/s;    646 sec
[2024-04-13 17:50:01,461 INFO] Step 4200/ 5000; xent: 5.67; lr: 0.0000309;  33 docs/s;    654 sec
[2024-04-13 17:50:08,945 INFO] Step 4250/ 5000; xent: 5.66; lr: 0.0000307;  32 docs/s;    661 sec
[2024-04-13 17:50:16,489 INFO] Step 4300/ 5000; xent: 5.74; lr: 0.0000305;  33 docs/s;    669 sec
[2024-04-13 17:50:24,180 INFO] Step 4350/ 5000; xent: 5.72; lr: 0.0000303;  33 docs/s;    676 sec
[2024-04-13 17:50:27,665 INFO] Loading train dataset from /home/wjm/hrx/BertSum_cnndm/bert_data/cnndm.train.32.bert.pt, number of examples: 1998
[2024-04-13 17:50:31,962 INFO] Step 4400/ 5000; xent: 5.67; lr: 0.0000302;  32 docs/s;    684 sec
[2024-04-13 17:50:39,649 INFO] Step 4450/ 5000; xent: 5.62; lr: 0.0000300;  33 docs/s;    692 sec
[2024-04-13 17:50:47,165 INFO] Step 4500/ 5000; xent: 5.69; lr: 0.0000298;  33 docs/s;    699 sec
[2024-04-13 17:50:54,839 INFO] Step 4550/ 5000; xent: 5.82; lr: 0.0000296;  33 docs/s;    707 sec
[2024-04-13 17:51:02,472 INFO] Step 4600/ 5000; xent: 5.77; lr: 0.0000295;  33 docs/s;    715 sec
[2024-04-13 17:51:10,152 INFO] Step 4650/ 5000; xent: 5.82; lr: 0.0000293;  33 docs/s;    722 sec
[2024-04-13 17:51:17,648 INFO] Step 4700/ 5000; xent: 6.00; lr: 0.0000292;  33 docs/s;    730 sec
[2024-04-13 17:51:25,354 INFO] Step 4750/ 5000; xent: 5.45; lr: 0.0000290;  33 docs/s;    737 sec
[2024-04-13 17:51:28,661 INFO] Loading train dataset from /home/wjm/hrx/BertSum_cnndm/bert_data/cnndm.train.111.bert.pt, number of examples: 2000
[2024-04-13 17:51:33,141 INFO] Step 4800/ 5000; xent: 5.63; lr: 0.0000289;  32 docs/s;    745 sec
[2024-04-13 17:51:40,748 INFO] Step 4850/ 5000; xent: 5.56; lr: 0.0000287;  33 docs/s;    753 sec
[2024-04-13 17:51:48,456 INFO] Step 4900/ 5000; xent: 5.59; lr: 0.0000286;  33 docs/s;    761 sec
[2024-04-13 17:51:56,094 INFO] Step 4950/ 5000; xent: 5.60; lr: 0.0000284;  33 docs/s;    768 sec
[2024-04-13 17:52:03,740 INFO] Step 5000/ 5000; xent: 5.81; lr: 0.0000283;  33 docs/s;    776 sec
[2024-04-13 17:52:03,741 INFO] Saving checkpoint /home/wjm/hrx/BertSum_cnndm/models/bert_classifier_5000/model_step_5000.pt
[2024-04-13 17:52:04,805 INFO] Loading train dataset from /home/wjm/hrx/BertSum_cnndm/bert_data/cnndm.train.9.bert.pt, number of examples: 1999
